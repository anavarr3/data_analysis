---
title: "data_analysis_foreclosures"
output: html_document
date: "2023-11-10"
data: https://opendata.maryland.gov/Housing/Maryland-Notices-of-Intent-to-Foreclose-by-Zip-Cod/ftsr-vapt

---
# Strongest story idea: Right now we believe the strongest story could lie in the income levels overlapping with the intent to foreclose notices or the unemployment overlap with the PG County zip codes. We did a lot of the joining work this milestone, so we don't have a complete idea yet. I'm also interested in Q5 and why there was a spike of notices around the springtime before dropping back down. We think the income levels overlapping with the intent to foreclose notices could be newsworthy if it turns out that income levels are trending downward with intent to foreclose notices trend up, and by how much. Or, is it the opposite and can people, while they have a decent income, just not afford rent/mortgages?

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#install.packages("zoo")
```

```{r}
library(tidyverse)
library(dplyr)
library(janitor)
library(zoo)
library(ggplot2)
library(tidycensus)
library(lubridate)
```
# Dataset: Maryland Notices of Intent to Foreclose (July 2021 - Oct. 2023)
```{r}
# Read in and clean the data
foreclosures <- read_csv("Maryland_Notices_of_Intent_to_Foreclose.csv") |> 
  clean_names() |> 
  rename(february_2022 = february_2020)
foreclosures
```
# Cleaning
In cleaning the data, we recognized that there was an error in the original data so we contacted the creator of the data to confirm that February 2020 was supposed to be marked February 2022. We made sure to rename the column after reading in the data and using clean_names() to take the spaces out and make the column names uniform. There wasnâ€™t any other obvious cleaning needed from what we can tell. Here's our quick map: https://datawrapper.dwcdn.net/M2Foe/2/

```{r}
# Here we wanted to try and make a quick map to see what we were looking at other than just a table of numbers.
foreclosures_sum <- foreclosures |>
  group_by(zip) |>
  summarise(foreclosures_sum = sum(across(contains("_")))) |>
  arrange(desc(foreclosures_sum))

foreclosures_sum

write_csv(foreclosures_sum, "foreclosure_sum.csv")
```
We decided to go question by question to get a better understanding of the limitations present and explore some strategies to overcome those limitations and answer the questions that we posed.

# Q1
From July '21- Oct'23 "the end of the pause on foreclosure notices", which zip code has had the most notices recorded?
**This question is pretty much addressed above by adding the number of notices across the data set, grouping the results by zip code, and arranging them in descending order. The zip code 20772 in Prince George's County (including Upper Marlboro) had the highest number over the course of this timespan. See this map for a quick vizualizeation: https://datawrapper.dwcdn.net/M2Foe/1/**

# Findings: We found that the zip code 20772 has the highest number of intent to foreclose notices, which is in PG County. We're currently trying to figure out how that compares to other zip codes. Transposing the data was the largest hurdle here and for the rest of the questions as we had to figure out how to format different aspects.
```{r}
foreclosures_sum |> 
  arrange(desc(foreclosures_sum))
```

#Q2 
How has have foreclosure notices in this zip code changed over time? How does this compare to the other zip codes?
**Since the dataset is already ordered chronologically, simply filtering for the zip code that had the most notices will give us an idea of how many notices were given in that zip code month-by-month. We might need to contact the data set owner again to see if he has the pre-pandemic data.**

#Findings: The number of intent to foreclose notices dramatically rose in this zip code. We decided to compare it to a zip code with a simarl amount of NOI and a similar median income (21206) as well as a few other zip codes from different median incomes and then charted the number of foreclosure notices over time. We saw that across the zip codes, there were consistent peaks in April 2022, Aug. 2022 and May 2023. In Aug. 2023, our chosen zip code (20772) and the other similar zip code (21206) peaked while the others slightly rose or fell.

#As for the overall aspect, the zip code 20772 had a percent change of 681.25%. That's a lot, but nowhere near the top in terms of which zip code had the highest percent change. When we filtered for only zip codes in Prince George's County, it has the 11th highest percent change out of 27 of PG counties with a calculated percent change. We actually found that there was one zip code, 20712, that saw a negative percent change -- it went from two notices in July 2021 to one notice in Oct. 2023. However, there were some years in between where it had higher amounts. One month it had a total of eight notices. This also doesn't take into account the zip codes with lower population compared to more populated zip codes. 

#The zip code with the highest percent change -- 20746 -- jumped from 1 foreclosure notice to 40 by October 2023. In comparison, 20772 jumped from 16 notices to 125.

```{r}
# We transposed the data set to tackle this question. We had a lot of questions for ChatGPT https://chat.openai.com/share/a3d690f6-baec-4fc7-beb2-4844daf93de6 since there was an issue with getting R to recognize the first column as the first column.

# Here we made another attempt at transposing to format the dates using lubridate instead of the zoo package. However we continued to run into the issue of getting a character column instead of a date. We were, however, able to solve our issue with the date column being in a weird month_year format by using the format function.
transposed_redo_2 <- foreclosures |>
  gather(date, value, -1) |>
  spread(key = zip, value = value) |>
  mutate(
    date= mdy(date),
    date=floor_date(date, "month")
    #month_year = format(date, "%B_%Y")
    ) |>
  arrange(date)
  #select(month_name, everything())
transposed_redo_2

#Now we're just cleaning a few things up. This dataframe is specific to the zip code 20772 for each month of the year from July 2021 to October 2023, so we just renamed the 20772 column "foreclosures" and kept the date and foreclosures columns before arranging by date. Because the date column is a character rather than an actual date, it's arranged in alphabetical order. We can work with that for now and separate them if we join later. 
top_foreclosure_zip <- transposed_redo_2 |> 
  rename(foreclosures = "20772") |> 
  select(date, foreclosures) |> 
  arrange(date)

top_foreclosure_zip

write_csv(top_foreclosure_zip, "top_foreclosure_zip.csv")
```

```{r}
#Here we decided to compare the 20772 zip code to a few others based on where they fell with median income levels and foreclosure numbers (see datawrapper chart for details).
foreclosures_zip <- transposed_redo_2 |>
select(date, "20772", "20878", "21122", "20640", "20895", "21206")  |>
arrange(date)
foreclosures_zip

write_csv(foreclosures_zip, "foreclosures_zip.csv")
```


```{r}
# Time to chart it! 

#datawrapper: https://datawrapper.dwcdn.net/g8fID/1/

top_foreclosure_zip |> 
   ggplot() + 
  geom_line(aes(x=date, y=foreclosures)) +
 labs(
    title="Foreclosure notices rose drastically in this PG County zip code",
    x = "month",
    y = "foreclosures notices issued",
    caption = "Source: Office of Financial Regulation") +
theme(
    axis.text.x = element_text(angle = 45,  hjust=1)) 
 #  geom_text(check_overlap = TRUE, size = 2, aes(label = date), hjust = 0.5, vjust = -0.5)
```


```{r}
foreclosures_zip_all <- transposed_redo_2 |>
arrange(date)

foreclosures_zip_all

write_csv(foreclosures_zip_all, "foreclosures_zip_all.csv")
```

```{r}
# I want to compare the above chart to the overall pattern. Here I am just readjusting the transposed_redo data set so I can see the month_year column at the front and get rid of the original date column

transposed_redo_arranged <- transposed_redo_2 |> 
#[, c(ncol(transposed_redo_2), 1:(ncol(transposed_redo_2)-1))] |> 
 select(-date)

transposed_redo_arranged
```


```{r}
#Okay, now that we've cleaned that up a bit, let's count the sum of the rows. This first part stores the dataframe inside a new dataframe without the first column so we can add a column (mutate) that is the sum of all the other columns without pulling the date into the calculation.

col_sum <- transposed_redo_arranged |> 
  mutate(sum_col = rowSums(transposed_redo_arranged[,-1]))

#This next line creates a new dataframe, where we take our fresh one ^ and ask it to pull the new column from the end (sum_col) and place it at the front.

sum_arranged <- col_sum[, c(ncol(transposed_redo_2), 1:(ncol(col_sum)-1))]

sum_arranged
```

```{r}
# Here I'm double checking that the first row is correct
rowSums(transposed_redo_arranged[1,-1])
```

```{r}
#I want to look at the percent change of notices from July 2021 and October 2023 since the moratorium on residential foreclosures in Maryland expired June 30, 2021 and foreclosures resumed July 1, 2021. This is for all the zip codes in Maryland. Farther down, we'll compare just PG County zip codes.

perc_change_foreclosures <- foreclosures |> 
  mutate(perc_change = ifelse(july_2021 != 0, (october_2023 - july_2021) / july_2021 * 100, NA)) |> 
  select(zip, perc_change) |> 
  arrange(desc(perc_change))

perc_change_foreclosures

write_csv(perc_change_foreclosures, "perc_change_foreclosures.csv")
```

```{r}
perc_change_pg_foreclosures <- foreclosures |> 
filter(zip %in% c ("20772", "20607", "20608", "20613", "20623", "20703", "20704", "20705", "20706", "20707", "20708", "20710", "20712", "20715", "20716", "20717", "20718", "20719", "20720", "20721", "20722", "20725", "20735", "20737", "20740", "20741", "20742", "20743", "20744", "20745", "20746", "20747", "20748", "20750", "20757", "20762", "20769", "20770", "20771", "20781", "20782", "20783", "20784", "20785", "20787", "20788")) |> 
  mutate(perc_change = ifelse(july_2021 != 0, (october_2023 - july_2021) / july_2021 * 100, NA)) |> 
  select(zip, perc_change) |> 
  arrange(desc(perc_change))

perc_change_pg_foreclosures
```

#Q3
Using census data, what are the demographics (income) of this zip code? How does this compare to other zip codes and their demographics? Is there a relationship between a zip code's median income and the number of foreclosures per capita?

#Findings: There very weak relationship between median income and notices per capita -- however, there is noticeably more variation in foreclosure notice rates in poorer zip codes compared to wealthier zip codes. The zip code 20772 falls around the middle of the median income range, yet has had the highest number of foreclosures compared to other zip codes. When we mapped out these zip codes, we found that zip codes near the border of Charles and PG County had a large cluster of zip codes with a high rate of foreclosure notices per capita. There were also some noticeable zip codes in Dorchester and Wicomico counties.

```{r}
#Here we are using the zip from the census data to pull the population by zip. We then rename geoid, estimate and moe to zip, population and moe_pop

acs5_variables <- load_variables(2021, "acs5", cache = TRUE)
acs5_variables

mdzip <- get_acs(geography="zcta", variables = "B02001_001", year=2021)
mdzip <- mdzip |>
      clean_names() |>
      #separate(name, c('county', 'state'), sep=",") |>
      rename(zip=geoid, population=estimate, moe_pop=moe) |>
      group_by(zip) |>
      arrange(desc(population))
mdzip

#Here we are now joining the foreclosures_sum dataframe (the one that shows the total number of foreclosures for each zip code) with the mdzip data frame (which we made above). We then selected only the zip, foreclosure_sum, population and moe_pop columns.
foreclosure_state_pop <- left_join(foreclosures_sum, mdzip, join_by(zip))
foreclosure_state_pop <- foreclosure_state_pop |> select(zip, foreclosures_sum, population, moe_pop)
foreclosure_state_pop

#Here we're pulling the income from the census
md_income <- get_acs(geography="zcta", variables = "B19013_001", year=2021)

md_income <- md_income |>
      clean_names() |>
      rename(zip=geoid, income=estimate, moe_inc=moe) |>
      group_by(zip) |>
      arrange(desc(income))
md_income

foreclosure_pop_income <- left_join(foreclosure_state_pop, md_income, join_by(zip)) |> 
  select(zip, foreclosures_sum, population, moe_pop, income, moe_inc)
foreclosure_pop_income 
```

```{r}
#We wanted to first plot out the sum of intent to foreclose notices by zip code.
ggplot(per_capita, aes(x=income, y=foreclosures_sum)) +
  geom_point()+
  labs(
    title="Zip codes with higher median incomes had less variety in NOI totals",
    x = "Median Income",
    y = "Sum of NOI",
    caption = "source: Source: Office of Financial Regulation") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    geom_text(check_overlap = TRUE, size = 2, aes(label = zip), hjust = 0.5, vjust = -0.5)
```

```{r}
#How spread out is our data? It looks like the zip codes lean under 150,000, which is why we decided to use the Kendall method for the correlation test. 

ggplot(per_capita, aes(x = income)) +
  geom_histogram(binwidth = 5, fill = "skyblue", color = "black") +
  labs(title = "Distribution of median income",
       x = "Median Income",
       y = "Frequency")
```

```{r}
#Here we are charting the relationship between a zip code's median income and the number of intent to foreclose notices.
per_capita |>
  ggplot() +
  geom_point(aes(x=income,y=foreclosures_sum)) +
  geom_smooth(aes(x=income,y=foreclosures_sum), method="lm")+
   labs(title = "Relationship between median income and sum of NOIs",
       x = "Median Income",
       y = "Sum of NOIs")
```

```{r}
# The Kendall Method shows that there isn't a strong relationship between these two.
cor.test(per_capita$income, per_capita$foreclosures_sum, method = "kendall")

```

```{r}
#Okay, now to plot the median income and the number of intent to foreclose notices by zip code.

ggplot(per_capita, aes(x=income, y=percapita)) +
  geom_point()+
  labs(
    title="A weak relationship exists between NOIs per capita and median income",
    x = "Median Income",
    y = "NOIs Per Capita",
    caption = "source: Source: Office of Financial Regulation") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    geom_text(check_overlap = TRUE, size = 2, aes(label = zip), hjust = 0.5, vjust = -0.5)
```

```{r}
per_capita |>
  ggplot() +
  geom_point(aes(x=income,y=percapita)) +
  geom_smooth(aes(x=income,y=percapita), method="lm")+
  labs(
    title="A weak relationship exists between NOIs per capita and median income",
    x = "Median Income",
    y = "NOIs Per Capita",
    caption = "source: Source: Office of Financial Regulation")
```

```{r}
#There very weak relationship between median income and notices per capita -- however, there is noticeably more variation in foreclosure notice rates in poorer zip codes compared to wealthier zip codes.

cor.test(per_capita$income, per_capita$percapita, method = "kendall")

```

Q4. How do the unemployment rates across PG County compare to the intent to foreclose notices for each month? 

#Findings: It seems like there is no relationship. As unemployment rates decrease following July 2021,intent to foreclose notices continue to rise. 

# New Join Data

```{r}
#Here we're loading the unemployment rate data, cleaning it and joining it to the transposed_redo_2 dataframe, joining it by the date column. We then selected the zip codes that were specific to PG County.

#Cleaning
pg_unemployment_rate <- read_csv("pg_unemployment_rate.csv") |>
  clean_names() |> 
  rename(pg_unemployment=mdprin5urn)
pg_unemployment_rate

#Here we are trying to isolate just the years 2021, 2022 and 2023
multiple_years <- c(2021, 2022, 2023)
pg_unemployment_rate <- pg_unemployment_rate |> 
  filter(as.numeric(str_extract(date, "\\d{4}")) %in% multiple_years)
pg_unemployment_rate

#We got "NA" for some rows before we realized the original data didn't include those months, so we took them out.
pg_unemployment_notices <- left_join(pg_unemployment_rate, transposed_redo_2, join_by(date)) |> 
  filter(date!="2021-01-01" & date!="2021-02-01" & date!="2021-03-01" & date!="2021-04-01" & date!="2021-05-01"& date!="2021-06-01") |> 
  group_by(date, pg_unemployment) |>
  summarize(across(c("20607", "20608", "20613", "20623", "20703", "20704", "20705", "20706", "20707", "20708", "20710", "20712", "20715", "20716", "20717", "20718", "20719", "20720", "20721", "20722", "20725", "20735", "20737", "20740", "20741", "20742", "20743", "20744", "20745", "20746", "20747", "20748", "20750", "20757", "20762", "20769", "20770", "20771", "20781", "20782", "20783", "20784", "20785", "20787", "20788", "20772"), sum)) |>
  mutate(pg_notices = rowSums(across(c("20607", "20608", "20613", "20623", "20703", "20704", "20705", "20706", "20707", "20708", "20710", "20712", "20715", "20716", "20717", "20718", "20719", "20720", "20721", "20722", "20725", "20735", "20737", "20740", "20741", "20742", "20743", "20744", "20745", "20746", "20747", "20748", "20750", "20757", "20762", "20769", "20770", "20771", "20781", "20782", "20783", "20784", "20785", "20787", "20788", "20772")))) |> 
  select(date, pg_unemployment, pg_notices)
pg_unemployment_notices

# These are the PG county zip codes that didn't exist in the original data: "20731", "20790", "20791", "20799", "20753", "20749", "20768", "20709", "20726", "20738", "20797", "20697", "20752", "20775", "20792"

write_csv(pg_unemployment_notices, "pg_unemployment_notices.csv")


```

```{r}
perc_change_pg_unemployment_notices <- pg_unemployment_notices |>
 arrange(date)


perc_change_pg_unemployment_notices
```

Q5. What are the differences in foreclosure notices by season (warm months vs cold months)? The only full year we have is 2022, so let's look at that year to start.
**We're concerned about our ability to filter by month since the dates are in the column position.**

#Findings so far: Again, having trouble getting the months to show up at the bottom of the chart, but we found there is an incline over the span of the year -- meaning there were more notices in the colder months. However, there was a pretty big spike in the spring (it looks like the spring) before the notices dropped again. I would be interested in finding out more about that.


```{r}
transposed_redo_2 <- foreclosures |>
  gather(date, value, -1) |>
  spread(key = zip, value = value) |>
  mutate(
    date= mdy(date),
    date=floor_date(date, "month")
    #month_year = format(date, "%B_%Y")
    ) |>
  arrange(date)
  #select(month_name, everything())
transposed_redo_2
```


```{r}
#Here we are selecting only the 2022 months
foreclosure_notices_2022 <- transposed_redo_2 |> 
  mutate(year = (year(date))) |> 
  filter(year == "2022") |> 
  select(!year)

foreclosure_notices_2022
```


```{r}
col_sum <- transposed_redo_arranged |> 
  mutate(sum_col = rowSums(transposed_redo_arranged[,-1]))

#This next line creates a new dataframe, where we take our fresh one ^ and ask it to pull the new column from the end (sum_col) and place it at the front.

sum_arranged <- col_sum[, c(ncol(transposed_redo_2), 1:(ncol(col_sum)-1))]

sum_arranged
```

#This next line creates a new dataframe, where we take our fresh one ^ and ask it to pull the new column from the end (sum_col) and place it at the front.

sum_arranged <- col_sum[, c(ncol(transposed_redo_2), 1:(ncol(col_sum)-1))]

sum_arranged






```{r}
# Now for the adding
final_foreclosures_2022 <- foreclosure_notices_2022[, c(ncol(foreclosure_notices_2022), 1:(ncol(foreclosure_notices_2022)-1))] |> 
  arrange(date)

final_foreclosures_2022
```





```{r}
col_sum_2022_months <- final_foreclosures_2022 |> 
  mutate(sum_of_notices = rowSums(final_foreclosures_2022[,-1]))

foreclosures_by_month_2022 <- col_sum_2022_months[, c(ncol(transposed_foreclosures_2022), 1:(ncol(col_sum_2022_months)-1))] |>
  select(date, sum_of_notices)

foreclosures_by_month_2022
```

```{r}
#Checking for the same number on the first row
rowSums(foreclosures_by_month_2022[1,-1])
```

```{r}
# Time to chart. I'm having trouble getting the ticks and month labels for this, and Chat GPT said it's because of the data type of the month_year column. We need to change that column to a date type using the first day of every month.

foreclosures_by_month_2022 |> 
  ggplot() + 
  geom_line(aes(x= month_year, y=sum_of_notices)) +
  scale_x_continuous(breaks = 1:52) +
 labs(
    title="Intent to foreclose notices in Maryland across 2022",
    x = "month",
    y = "intent to foreclose notices issued",
    caption = "Source: Office of Financial Regulation") +
theme(
    axis.text.x = element_text(angle = 45,  hjust=1)) 

```
